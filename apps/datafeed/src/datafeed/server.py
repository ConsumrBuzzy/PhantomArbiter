"""
Data Feed Server - gRPC server for market data streaming.

Front-of-line market data ingestion engine.
"""

from __future__ import annotations

import asyncio
import os
import time
from concurrent import futures
from typing import Iterator, Dict, Any, Optional, List
import logging

# Configure Logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(name)s] %(levelname)s: %(message)s")
logger = logging.getLogger("DataFeedServer")

try:
    import grpc
    from grpc import aio as grpc_aio
except ImportError:
    grpc = None  # type: ignore
    grpc_aio = None  # type: ignore

from datafeed.price_aggregator import (
    PriceAggregator,
    PricePoint,
    PriceSource,
    get_aggregator,
)
from datafeed.websocket_manager import (
    WebSocketManager,
    WssConfig,
    get_websocket_manager,
)
from apps.datafeed.src.scrapers.dexscreener import DexScreenerScraper
from apps.datafeed.src.scrapers.birdeye import BirdeyeScraper


# --- Configuration ---
DATAFEED_PORT = int(os.getenv("DATAFEED_PORT", "9000"))
DATAFEED_HOST = os.getenv("DATAFEED_HOST", "0.0.0.0")


# --- Generated protobuf stubs (placeholder until compiled) ---
# These would be generated by: python -m grpc_tools.protoc ...
# For now, we use a simplified implementation

class MarketDataServicer:
    """
    gRPC service implementation for MarketDataService.
    
    Provides:
    - StreamPrices: Real-time price streaming
    - GetSnapshot: Current market state
    - SubscribeTopic: Topic-based subscriptions
    - HealthCheck: Service health
    """
    
    def __init__(self, aggregator: PriceAggregator) -> None:
        self.aggregator = aggregator
        self._start_time = time.time()
        self._subscribers: Dict[str, asyncio.Queue] = {}
    
    async def StreamPrices(self, request, context) -> None:
        """Stream real-time price updates to client."""
        client_id = f"client_{id(context)}_{time.time()}"
        queue: asyncio.Queue = asyncio.Queue(maxsize=1000)
        
        # Subscribe to aggregator updates
        async def on_update(point: PricePoint):
            try:
                queue.put_nowait(point)
            except asyncio.QueueFull:
                # Drop oldest if full
                try:
                    queue.get_nowait()
                    queue.put_nowait(point)
                except:
                    pass
        
        self.aggregator.subscribe_async(on_update)
        self._subscribers[client_id] = queue
        
        try:
            while True:
                point = await queue.get()
                # Convert to response (would use protobuf message)
                yield self._point_to_response(point)
        except asyncio.CancelledError:
            pass
        finally:
            self.aggregator.unsubscribe(on_update)
            del self._subscribers[client_id]
    
    async def GetSnapshot(self, request, context):
        """Get current market snapshot."""
        max_age = getattr(request, 'max_age_seconds', 30)
        points = await self.aggregator.get_snapshot(max_age)
        
        return {
            "prices": [self._point_to_dict(p) for p in points],
            "snapshot_time_ms": int(time.time() * 1000),
            "source_count": len(set(p.source for p in points)),
        }
    
    async def HealthCheck(self, request, context):
        """Return service health status."""
        stats = self.aggregator.get_stats()
        wss_manager = get_websocket_manager()
        
        return {
            "status": "OK",
            "wss_connections": wss_manager.get_connection_count(),
            "symbols_tracked": stats.symbols_tracked,
            "avg_latency_ms": stats.avg_latency_ms,
            "uptime_seconds": int(time.time() - self._start_time),
        }
    
    def _point_to_response(self, point: PricePoint) -> Dict[str, Any]:
        """Convert PricePoint to gRPC response dict."""
        return point.to_dict()
    
    def _point_to_dict(self, point: PricePoint) -> Dict[str, Any]:
        """Convert PricePoint to dict."""
        return point.to_dict()


class DataFeedServer:
    """
    Main Data Feed server.
    
    Orchestrates:
    - gRPC server for client connections
    - WebSocket manager for data sources
    - Price aggregator for state management
    """
    
    def __init__(self) -> None:
        self.aggregator = get_aggregator()
        self.wss_manager = get_websocket_manager()
        self._server = None
        self._running = False
        
        # Initialize Scrapers
        # TODO: Load mints dynamically from a config/shared file
        self.monitored_mints = [
            "JUPyiwrYJFskUPiHa7hkeR8VUtAeFoSYbKedZNsDvCN", # JUP
            "EKpQGSJtjMFqKZ9KQanSqYXRcF8fBopzLHYxdM65zcjm", # WIF
            "DezXAZ8z7PnrnRJjz3wXBoRgixCa6xjnB7YaB1pPB263", # BONK
            "HeLp6NuQkmYB4pYWo2zYs22mESHXPQHz5bB9d4qDpump", # HELIUS
        ]
        
        self.scrapers = [
            DexScreenerScraper(self.monitored_mints),
            BirdeyeScraper(self.monitored_mints)
        ]

        # Register callbacks
        for scraper in self.scrapers:
            scraper.register_callback(self._on_scraper_update)

    async def _on_scraper_update(self, data: Dict[str, Any]):
        """Handle normalized data from scrapers."""
        try:
            point = PricePoint(
                symbol=data.get("token")[:8], # temp symbol
                mint=data["token"],
                price=data["price"],
                volume_24h=data.get("metadata", {}).get("volume_24h", 0),
                liquidity=data.get("metadata", {}).get("liquidity", 0),
                timestamp_ms=data["timestamp"],
                source=PriceSource.HTTP,
                slot=0
            )
            # Use fire-and-forget for async updates to avoid blocking scraper loop
            asyncio.create_task(self.aggregator.update_price(point))
        except Exception as e:
            logger.error(f"Failed to process scraper update: {e}")
    
    def configure_wss_sources(self) -> None:
        """Configure WebSocket data sources."""
        # Helius configuration (would come from settings)
        helius_key = os.getenv("HELIUS_API_KEY", "")
        if helius_key:
            self.wss_manager.add_connection(WssConfig(
                name="HELIUS",
                url=f"wss://atlas-mainnet.helius-rpc.com/?api-key={helius_key}",
                subscriptions=[],  # Will add subscriptions later
            ))
        
        # Set up message handler
        self.wss_manager.set_message_callback(self._on_wss_message)
    
    def _on_wss_message(self, source: str, data: Dict[str, Any]) -> None:
        """Handle incoming WebSocket message."""
        # Parse based on source format
        # This would be extended for each data source
        try:
            # Generic parsing - actual implementation depends on source
            if "price" in data and "mint" in data:
                point = PricePoint(
                    symbol=data.get("symbol", data["mint"][:8]),
                    mint=data["mint"],
                    price=float(data["price"]),
                    volume_24h=float(data.get("volume_24h", 0)),
                    liquidity=float(data.get("liquidity", 0)),
                    timestamp_ms=int(time.time() * 1000),
                    source=PriceSource.WSS,
                    slot=int(data.get("slot", 0)),
                )
                # Fire and forget async update
                asyncio.create_task(self.aggregator.update_price(point))
        except Exception:
            pass
    
    async def start(self) -> None:
        """Start the Data Feed server."""
        print(f"ðŸŒ [DataFeed] Starting server on {DATAFEED_HOST}:{DATAFEED_PORT}")
        
        self._running = True
        
        # Start Scrapers
        logger.info(f"Starting {len(self.scrapers)} Scrapers...")
        for scraper in self.scrapers:
            await scraper.start()

        # Configure and start WSS connections
        self.configure_wss_sources()
        await self.wss_manager.start()
        
        # Start gRPC server (simplified - would use actual grpc.aio.server)
        print(f"âœ… [DataFeed] gRPC server ready on :{DATAFEED_PORT}")
        print(f"ðŸ“Š [DataFeed] Price aggregator initialized")
        
        # Keep running
        try:
            while self._running:
                await asyncio.sleep(1)
        except asyncio.CancelledError:
            await self.stop()

    async def stop(self):
        """Graceful shutdown."""
        logger.info("Stopping DataFeed Service...")
        self._running = False
        
        for scraper in self.scrapers:
            await scraper.stop()
            
        await self.wss_manager.stop()
        await self.aggregator.stop()

        """Stop the Data Feed server."""
        print("ðŸ›‘ [DataFeed] Shutting down...")
        self._running = False
        await self.wss_manager.stop()
        print("âœ… [DataFeed] Shutdown complete")
    
    def get_servicer(self) -> MarketDataServicer:
        """Get the gRPC servicer instance."""
        return MarketDataServicer(self.aggregator)


# --- Entry Point ---

async def serve() -> None:
    """Main server entry point."""
    server = DataFeedServer()
    
    try:
        await server.start()
    except KeyboardInterrupt:
        pass
    finally:
        await server.stop()


def main() -> None:
    """CLI entry point."""
    print("=" * 50)
    print("  PHANTOM DATA FEED ENGINE")
    print("  Front-of-Line Market Data Ingestion")
    print("=" * 50)
    
    asyncio.run(serve())


if __name__ == "__main__":
    main()
